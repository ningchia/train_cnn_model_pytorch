import torch
import torch.nn as nn
import os
import sys
import time
from typing import Literal

# --- å°å…¥ torchao çš„é‡åŒ– API ---
from torchao.quantization import quantize_ # æœ€ä¸Šå±¤çš„é‡åŒ–å‡½å¼
from torchao.quantization import Int8DynamicActivationInt8WeightConfig # Int8 å‹•æ…‹é‡åŒ–é…ç½®
# -----------------------------

# å‡è¨­ MobileNetTransfer çµæ§‹å®šç¾©åœ¨ cnn_models.py æˆ– model_defs.py ä¸­
# é€™è£¡æˆ‘å€‘éœ€è¦å°å…¥å®ƒ
# from model_defs import MobileNetTransfer 
# ç‚ºäº†è®“è…³æœ¬ç¨ç«‹é‹è¡Œï¼Œæˆ‘å€‘å¯ä»¥åœ¨æ­¤é‡æ–°å®šç¾©ï¼Œä½†å»ºè­°ä½¿ç”¨å°å…¥ã€‚
# å‡è¨­æ‚¨å·²å°‡ MobileNetTransfer æ”¾åœ¨ cnn_models.py æˆ– model_defs.py ä¸­
try:
    from model_defs import MobileNetTransfer 
except ImportError:
    # å¦‚æœå°å…¥å¤±æ•—ï¼Œé€™è£¡æ”¾å…¥ä¸€å€‹è‡¨æ™‚çš„çµæ§‹å®šç¾©ï¼ˆéœ€èˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰
    # è«‹ç¢ºä¿æ‚¨çš„ MobileNetTransfer çµæ§‹èˆ‡è¨“ç·´æ™‚ä¸€è‡´
    print("WARNING: model_defs.MobileNetTransfer å°å…¥å¤±æ•—ï¼Œè«‹ç¢ºä¿æ–‡ä»¶å­˜åœ¨ã€‚")
    # é€™è£¡çœç•¥äº† MobileNetTransfer çš„å®Œæ•´å®šç¾©ï¼Œå‡è¨­å®ƒå·²è¢«æ­£ç¢ºå°å…¥ã€‚
    pass


# --- 1. é…ç½®èˆ‡åƒæ•¸è¨­å®š (å¿…é ˆèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´) ---
MODEL_SAVE_PATH = "trained_model"
# è¼¸å…¥çš„ FP32 æª¢æŸ¥é»
FP32_CHECKPOINT_FILE = "latest_checkpoint_cifar10_mobilenet.pth"
FP32_CHECKPOINT_PATH = os.path.join(MODEL_SAVE_PATH, FP32_CHECKPOINT_FILE)

# è¼¸å‡ºçš„ INT8 æ¨¡å‹æª”æ¡ˆåç¨±
INT8_MODEL_FILE = "quantized_mobilenet_cifar10_int8.pth"
INT8_MODEL_PATH = os.path.join(MODEL_SAVE_PATH, INT8_MODEL_FILE)

NUM_CLASSES = 10 
DEVICE = torch.device("cpu") # é‡åŒ–é€šå¸¸åœ¨ CPU ä¸Šé€²è¡Œï¼Œä¸” INT8 æ¨¡å‹ä¸»è¦ç”¨æ–¼ CPU éƒ¨ç½²

# --- 2. æ¨¡å‹è¼‰å…¥å‡½å¼ ---
def load_fp32_model(num_classes: int) -> MobileNetTransfer:
    """è¼‰å…¥ MobileNetTransfer æ¨¡å‹çµæ§‹ä¸¦è¼‰å…¥ FP32 æ¬Šé‡ã€‚"""
    
    # å¿…é ˆä½¿ç”¨ use_pretrained=Falseï¼Œå› ç‚ºæˆ‘å€‘è¦è¼‰å…¥æœ¬åœ°è¨“ç·´å¥½çš„æ¬Šé‡
    model = MobileNetTransfer(num_classes=num_classes, use_pretrained=False) 
    model.to(DEVICE)

    if not os.path.exists(FP32_CHECKPOINT_PATH):
        raise FileNotFoundError(f"éŒ¯èª¤: æ‰¾ä¸åˆ° FP32 æª¢æŸ¥é»æª”æ¡ˆ {FP32_CHECKPOINT_PATH}ã€‚è«‹å…ˆé‹è¡Œè¨“ç·´è…³æœ¬ã€‚")
        
    checkpoint = torch.load(FP32_CHECKPOINT_PATH, map_location=DEVICE)
    
    try:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æˆåŠŸè¼‰å…¥ FP32 æ¨¡å‹æ¬Šé‡ (Epoch: {checkpoint['epoch']}, Acc: {checkpoint['best_accuracy']:.2f}%)")
    except Exception as e:
        raise ValueError(f"éŒ¯èª¤: è¼‰å…¥æ¨¡å‹æ¬Šé‡å¤±æ•—ã€‚è«‹ç¢ºèªæ¨¡å‹çµæ§‹èˆ‡æª¢æŸ¥é»æ˜¯å¦åŒ¹é…ã€‚\néŒ¯èª¤è¨Šæ¯: {e}")

    model.eval() # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
    return model

# --- 3. é‡åŒ–æµç¨‹ä¸»å‡½å¼ ---
def quantize_model():
    print("--- å•Ÿå‹•æ¨¡å‹é‡åŒ–æµç¨‹ ---")
    
    try:
        # æ­¥é©Ÿ 1: è¼‰å…¥ FP32 æ¨¡å‹
        fp32_model = load_fp32_model(NUM_CLASSES)
        
        # æ­¥é©Ÿ 2: å®šç¾©é‡åŒ–é…ç½® (Int8 Dynamic Quantization)
        # æ­¤é…ç½®æœƒå°‡æ¬Šé‡è½‰ç‚º Int8ï¼Œä¸¦åœ¨æ¨è«–æ™‚å‹•æ…‹é‡åŒ–æ¿€æ´»å€¼ã€‚
        quant_config = Int8DynamicActivationInt8WeightConfig()

        # æ­¥é©Ÿ 3: åŸ·è¡Œå¾Œè¨“ç·´å‹•æ…‹é‡åŒ– (Post-Training Dynamic Quantization)
        # é€™ç¨®æ–¹æ³•æœƒå°‡æ¬Šé‡å¾ FP32 è½‰æ›ç‚º INT8ï¼Œä¸¦åœ¨æ¨è«–æ™‚å‹•æ…‹æ ¡æº–æ¿€æ´»å€¼ã€‚
        print("\nâ³ æ­£åœ¨åŸ·è¡Œå¾Œè¨“ç·´å‹•æ…‹é‡åŒ– (FP32 -> INT8) ä½¿ç”¨ quantize_ å‡½å¼...")
        start_time = time.time()
        
        # **ä¸»è¦ä¿®æ”¹é»**: ä½¿ç”¨ quantize_ æ­é…é…ç½®
        # quantize_ æ˜¯ in-place å‡½å¼ï¼Œæœƒç›´æ¥ä¿®æ”¹ fp32_model
        quantize_(fp32_model, quant_config)
        
        quantized_model = fp32_model 
        
        end_time = time.time()
        print(f"âœ… é‡åŒ–å®Œæˆï¼è€—æ™‚: {end_time - start_time:.2f} ç§’")
        
        # æ­¥é©Ÿ 4: å„²å­˜é‡åŒ–å¾Œçš„ INT8 æ¨¡å‹
        torch.save(quantized_model.state_dict(), INT8_MODEL_PATH)

        # æ­¥é©Ÿ 5: é©—è­‰æª”æ¡ˆå¤§å°å’Œæº–ç¢ºåº¦å·®ç•° (å¯é¸ï¼Œä½†å¼·çƒˆæ¨è–¦)
        fp32_size = os.path.getsize(FP32_CHECKPOINT_PATH)
        int8_size = os.path.getsize(INT8_MODEL_PATH)

        print("-" * 40)
        print(f"FP32 æ¨¡å‹å¤§å°: {fp32_size / (1024**2):.2f} MB")
        print(f"INT8 æ¨¡å‹å¤§å°: {int8_size / (1024**2):.2f} MB")
        print(f"æª”æ¡ˆå¤§å°ç¸®æ¸›æ¯”ä¾‹: {fp32_size / int8_size:.2f} å€")
        print(f"\nğŸ‰ INT8 æ¨¡å‹å·²æˆåŠŸå„²å­˜åˆ°: {INT8_MODEL_PATH}")
        print("-" * 40)
        
        # æ³¨æ„ï¼šè¦æ¸¬è©¦ INT8 æ¨¡å‹çš„å¯¦éš›æ¨è«–æº–ç¢ºåº¦ï¼Œéœ€è¦ä½¿ç”¨å°ˆé–€çš„ INT8 æ¨¡å‹è¼‰å…¥å’Œæ¸¬è©¦è…³æœ¬ã€‚
        
    except FileNotFoundError as e:
        print(f"\n[éŒ¯èª¤] {e}")
        print("è«‹ç¢ºèª FP32 æª¢æŸ¥é»è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
    except ValueError as e:
        print(f"\n[éŒ¯èª¤] {e}")
        print("è«‹ç¢ºèª MobileNetTransfer é¡åˆ¥çš„å®šç¾©æ˜¯å¦æ­£ç¢ºã€‚")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n[ä¸€èˆ¬éŒ¯èª¤] é‡åŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == '__main__':
    quantize_model()