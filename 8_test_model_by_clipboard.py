import torch
import torch.nn as nn
from torchvision import transforms
import cv2
import numpy as np
import os
import sys
from PIL import Image, ImageGrab # ImageGrab ç”¨æ–¼å‰ªè²¼ç°¿
from typing import List, Optional, Any

# --- å°å…¥æ¨¡å‹çµæ§‹ (å‡è¨­ MobileNetTransfer å·²åœ¨å…¶ä¸­) ---
from model_defs import MobileNetTransfer 

# --- 1. é…ç½®èˆ‡åƒæ•¸è¨­å®š (å¿…é ˆèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´) ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_SAVE_PATH = "trained_model"

# ä½¿ç”¨ CIFAR-10 é·ç§»å­¸ç¿’çš„æª¢æŸ¥é»
CHECKPOINT_FILE = "latest_checkpoint_cifar10_mobilenet.pth"
CHECKPOINT_PATH = os.path.join(MODEL_SAVE_PATH, CHECKPOINT_FILE)

NUM_CLASSES = 10 
# CIFAR-10 é¡åˆ¥åç¨±
CLASS_NAMES = [
    "plane", "car", "bird", "cat", "deer", 
    "dog", "frog", "horse", "ship", "truck"
]

# --- è¼”åŠ©å‡½å¼ï¼šç”Ÿæˆæç¤ºç•«é¢ ---
def create_info_image(text: str, size: tuple = (400, 600)) -> np.ndarray:
    """ å‰µå»ºä¸€å€‹é»‘è‰²èƒŒæ™¯ï¼Œå¸¶æœ‰æŒ‡å®šæ–‡å­—çš„ OpenCV åœ–åƒã€‚ """
    height, width = size
    # å‰µå»ºé»‘è‰²åœ–åƒ
    img = np.zeros((height, width, 3), dtype=np.uint8) 
    
    # è¨ˆç®—æ–‡å­—ä½ç½®
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7
    font_thickness = 2
    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]
    
    # å±…ä¸­æ”¾ç½®
    text_x = (width - text_size[0]) // 2
    text_y = (height + text_size[1]) // 2 
    
    cv2.putText(img, text, (text_x, text_y), 
                font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)
    
    return img

# --- 2. å½±åƒé è™•ç†å‡½å¼ ---
def get_transform():
    """ç²å– MobileNetV2 æ¨è«–æ¨™æº–åŒ–çš„å‰è™•ç†çµ„åˆã€‚"""
    return transforms.Compose([
        transforms.Resize((224, 224)), 
        transforms.ToTensor(), 
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 
    ])

# --- 3. æ¨¡å‹è¼‰å…¥å‡½å¼ (ä¿æŒä¸è®Š) ---
def load_model(num_classes: int):
    """è¼‰å…¥ MobileNetTransfer æ¨¡å‹çµæ§‹ä¸¦è¼‰å…¥æ¬Šé‡ã€‚"""
    
    model = MobileNetTransfer(num_classes=num_classes, use_pretrained=False) 
    model.to(device)

    if not os.path.exists(CHECKPOINT_PATH):
        raise FileNotFoundError(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª¢æŸ¥é»æª”æ¡ˆ {CHECKPOINT_PATH}ã€‚è«‹å…ˆé‹è¡Œè¨“ç·´è…³æœ¬ã€‚")
        
    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
    
    try:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹æ¬Šé‡ (Epoch: {checkpoint['epoch']}, Acc: {checkpoint['best_accuracy']:.2f}%)")
    except Exception as e:
        raise ValueError(f"éŒ¯èª¤: è¼‰å…¥æ¨¡å‹æ¬Šé‡å¤±æ•—ã€‚è«‹ç¢ºèªæ¨¡å‹çµæ§‹èˆ‡æª¢æŸ¥é»æ˜¯å¦åŒ¹é…ã€‚\néŒ¯èª¤è¨Šæ¯: {e}")

    model.eval() 
    return model

# --- 4. åœ–ç‰‡è™•ç†èˆ‡æ¨è«–å‡½å¼ (ä¿®æ­£ RGBA è½‰æ›èˆ‡å­—é«”ç¸®æ”¾) ---
def inference_on_image(model: nn.Module, pil_image: Image.Image, transform: transforms.Compose, class_names: List[str]):
    """å° PIL Image é€²è¡Œæ¨è«–ï¼Œä¸¦å°‡çµæœç¹ªè£½åˆ°åœ–ç‰‡ä¸Šã€‚"""

    # æ­¤è™•è¦æ³¨æ„clipboardè£¡å–åˆ°çš„å½±åƒæ ¼å¼, æœ‰äº›æ˜¯RGBA, æœ‰äº›æ˜¯RGB.
    # å¦‚æœæ˜¯RGBA, æœƒé€ æˆ RuntimeError: The size of tensor a (4) must match the size of tensor b (3)... 
    # é€™å€‹éŒ¯èª¤ç™¼ç”Ÿåœ¨ torchvision.transforms.functional.py çš„ normalize å‡½å¼ä¸­ï¼Œ
    # ç•¶å®ƒå˜—è©¦åŸ·è¡Œ tensor.sub_(mean).div_(std) (å¼µé‡æ¸›å»å‡å€¼å†é™¤ä»¥æ¨™æº–å·®) æ™‚ã€‚
    #   Tensor A (4): æŒ‡çš„æ˜¯è¼¸å…¥å¼µé‡çš„ç¬¬ä¸€å€‹ç¶­åº¦ (é€šé“æ•¸)ã€‚
    #                 ç•¶æ‚¨å¾å‰ªè²¼ç°¿æˆ–æŸäº› PNG æª”æ¡ˆä¸­è®€å–åœ–ç‰‡æ™‚ï¼Œå®ƒå€‘å¯èƒ½åŒ…å« 4 å€‹é€šé“ï¼šR (ç´…)ã€G (ç¶ )ã€B (è—) å’Œ A (Alphaï¼Œé€æ˜åº¦)ã€‚
    #   Tensor B (3): æŒ‡çš„æ˜¯æ‚¨åœ¨ transforms.Normalize ä¸­å®šç¾©çš„ mean å’Œ std åˆ—è¡¨çš„é•·åº¦ï¼šmean=[0.485, 0.456, 0.406] (3å€‹å€¼)ã€‚
    # MobileNetV2 æ¨¡å‹æ˜¯é‡å° ImageNet è¨“ç·´çš„ï¼ŒImageNet åœ–ç‰‡éƒ½æ˜¯æ¨™æº–çš„ RGB ä¸‰é€šé“åœ–åƒã€‚
    # ç•¶ç¨‹å¼å˜—è©¦ç”¨ 3 å€‹å€¼çš„ mean å»æ¸›å» 4 å€‹é€šé“çš„è¼¸å…¥å¼µé‡æ™‚ï¼Œå°±æœƒç”¢ç”Ÿé€™å€‹éŒ¯èª¤ã€‚
    # è§£æ±ºæ–¹æ¡ˆï¼šåœ¨å°‡ PIL åœ–åƒè½‰æ›ç‚º NumPy é™£åˆ—ä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å¼·åˆ¶ PIL åœ–åƒçš„æ ¼å¼ç‚º RGB ä¸‰é€šé“ï¼Œå³ä½¿å®ƒåŸæœ¬æ˜¯ RGBA å››é€šé“ã€‚

    # å¼·åˆ¶å°‡è¼¸å…¥åœ–ç‰‡è½‰æ›ç‚º RGB ä¸‰é€šé“
    pil_image = pil_image.convert('RGB')
    
    # 1. å½±åƒé è™•ç†
    input_tensor = transform(pil_image)
    input_batch = input_tensor.unsqueeze(0).to(device) 

    # 2. æ¨è«–
    with torch.no_grad():
        output = model(input_batch)
    
    # 3. çµæœè§£ç¢¼
    probabilities = torch.nn.functional.softmax(output[0], dim=0)
    confidence, predicted_index = torch.max(probabilities, 0)
    
    predicted_class = class_names[predicted_index.item()]
    confidence_percent = confidence.item() * 100

    # 4. è½‰æ›å› OpenCV æ ¼å¼ (ç”¨æ–¼é¡¯ç¤º)
    cv_image = np.array(pil_image) 
    cv_image = cv2.cvtColor(cv_image, cv2.COLOR_RGB2BGR)

    # 5. ç¹ªè£½çµæœ
    text = f"Class: {predicted_class} | Conf: {confidence_percent:.2f}%"
    
    # ä½¿ç”¨ cv_image çš„å½¢ç‹€ä¾†è¨ˆç®— font_scale
    font_scale = max(0.6, cv_image.shape[0] / 500)
    
    cv2.putText(cv_image, text, (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), 2, cv2.LINE_AA)
    
    return cv_image, predicted_class, confidence_percent

# --- 5. ä¸»åŸ·è¡Œå€å¡Š (æŒçºŒè¿´åœˆï¼Œå¢å¼·å‰ªè²¼ç°¿è™•ç†) ---
def main():
    
    # --- ä½¿ç”¨è€…è¼¸å…¥é…ç½® ---
    IMAGE_PATH = "" 
    USE_CLIPBOARD = True
    # -----------------------
    
    try:
        # æ­¥é©Ÿ 1: è¼‰å…¥æ¨¡å‹å’Œå‰è™•ç†å™¨
        model = load_model(NUM_CLASSES)
        data_transform = get_transform()
        
        display_image = create_info_image("Waiting for Input...") # åˆå§‹æç¤ºç•«é¢
        last_clipboard_image_repr: Any = None # ç”¨æ–¼è¿½è¹¤å‰ªè²¼ç°¿å…§å®¹æ˜¯å¦æ”¹è®Š/è¢«è™•ç†
        
        print("\n--- æŒçºŒåœ–ç‰‡è¾¨è­˜å·²å•Ÿå‹• (æŒ‰ 'q' é€€å‡º) ---")

        # æ­¥é©Ÿ 2: æŒçºŒè¿´åœˆé€²è¡Œæ¨è«–
        while True:
            pil_image = None
            should_infer = False
            
            # A. å„ªå…ˆæª¢æŸ¥æª”æ¡ˆè·¯å¾‘ (å¦‚æœè¨­å®šäº†ï¼Œä¸”æª”æ¡ˆå­˜åœ¨)
            if IMAGE_PATH and os.path.exists(IMAGE_PATH):
                try:
                    # æª”æ¡ˆæ¨¡å¼ï¼šæ¯æ¬¡é‡æ–°è®€å–æª”æ¡ˆ
                    pil_image = Image.open(IMAGE_PATH)
                    should_infer = True
                except Exception:
                    # å¦‚æœæª”æ¡ˆè®€å–å¤±æ•—ï¼Œé¡¯ç¤ºéŒ¯èª¤æç¤º
                    display_image = create_info_image(f"File Error: {IMAGE_PATH}")
                    
            # B. å…¶æ¬¡æª¢æŸ¥å‰ªè²¼ç°¿
            elif USE_CLIPBOARD:
                try:
                    current_clipboard_content = ImageGrab.grabclipboard() 
                    
                    # åªæœ‰ç•¶å…§å®¹èˆ‡ä¸Šæ¬¡ä¸åŒæ™‚æ‰è™•ç†
                    if current_clipboard_content != last_clipboard_image_repr:
                        
                        last_clipboard_image_repr = current_clipboard_content
                        
                        # Case 1: å‰ªè²¼ç°¿å…§å®¹æ˜¯ PIL Image (ç›´æ¥çš„åœ–åƒæ•¸æ“šï¼Œå¦‚æˆªåœ–)
                        if isinstance(current_clipboard_content, Image.Image):
                            pil_image = current_clipboard_content
                            should_infer = True
                            
                        # Case 2: å‰ªè²¼ç°¿å…§å®¹æ˜¯æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ (å¦‚è¤‡è£½äº†åœ–ç‰‡æª”æ¡ˆ)
                        elif isinstance(current_clipboard_content, list) and len(current_clipboard_content) > 0:
                            first_item = current_clipboard_content[0]
                            if isinstance(first_item, str) and os.path.isfile(first_item) and first_item.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                                pil_image = Image.open(first_item)
                                should_infer = True
                                print(f"ğŸ–¼ï¸ è¼‰å…¥æª”æ¡ˆè·¯å¾‘: {first_item}   ", end='\r')
                                
                            else:
                                # åˆ—è¡¨ä½†ä¸æ˜¯åœ–ç‰‡æª”æ¡ˆ
                                display_image = create_info_image(f"Clipboard Type: list (unsupported content)")
                                print(f"ğŸ“ å‰ªè²¼ç°¿å…§å®¹: list (unsupported)   ", end='\r')
                                
                        # Case 3: å‰ªè²¼ç°¿æ˜¯å…¶ä»–é¡å‹ (å¦‚ç´”æ–‡æœ¬ã€None)
                        else:
                            content_type_str = type(current_clipboard_content).__name__
                            display_image = create_info_image(f"Clipboard Type: {content_type_str}")
                            print(f"ğŸ“ å‰ªè²¼ç°¿å…§å®¹: {content_type_str}   ", end='\r')
                            
                except Exception as e:
                    # å‰ªè²¼ç°¿è®€å–éŒ¯èª¤ï¼Œå¯èƒ½æ˜¯æ¬Šé™æˆ–æ ¼å¼å•é¡Œ
                    display_image = create_info_image(f"Clipboard Read Error: {e.__class__.__name__}")
                    print(f"âŒ å‰ªè²¼ç°¿è®€å–éŒ¯èª¤: {e.__class__.__name__}   ", end='\r')
            
            # C. å¦‚æœéœ€è¦æ¨è«– (å¯èƒ½æ˜¯æ–°è¼‰å…¥çš„æª”æ¡ˆæˆ–æ–°è²¼çš„åœ–ç‰‡)
            if should_infer and pil_image is not None:
                try:
                    result_cv_image, predicted_class, confidence_percent = inference_on_image(
                        model, pil_image, data_transform, CLASS_NAMES
                    )
                    
                    # æ›´æ–°é¡¯ç¤ºåœ–ç‰‡
                    display_image = result_cv_image
                    
                    # åœ¨å‘½ä»¤åˆ—è¼¸å‡ºçµæœ (ä½¿ç”¨ \r ä¾†è¦†è“‹å‰ä¸€è¡Œ)
                    print(f"ğŸ‘ï¸ é æ¸¬: {predicted_class} | ç½®ä¿¡åº¦: {confidence_percent:.2f}%   ", end='\r')
                
                except Exception as e:
                    # æ¨è«–éç¨‹ä¸­çš„éŒ¯èª¤ (ä¾‹å¦‚åœ–ç‰‡æå£)
                    display_image = create_info_image(f"Inference Error: {e.__class__.__name__}")
                    print(f"ğŸ”¥ æ¨è«–éŒ¯èª¤: {e.__class__.__name__}   ", end='\r')

            # D. é¡¯ç¤ºçµæœ
            cv2.imshow('Inference Result (Press "q" to quit)', display_image)
            
            # E. æª¢æŸ¥æŒ‰éµ (ä½¿ç”¨ cv2.waitKey(10) ç¢ºä¿è¶³å¤ é«˜çš„å¹€ç‡å’ŒæŒ‰éµéŸ¿æ‡‰)
            key = cv2.waitKey(10) & 0xFF
            if key == ord('q'):
                break

    except (FileNotFoundError, ValueError) as e:
        print(f"\n[è‡´å‘½éŒ¯èª¤] {e}") 
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n[ä¸€èˆ¬éŒ¯èª¤] ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    finally:
        cv2.destroyAllWindows()


if __name__ == '__main__':
    main()