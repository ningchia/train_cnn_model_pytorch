import torch
import torch.nn as nn
# --- å°å…¥æ¨¡å‹çµæ§‹: ä½¿ç”¨ MobileNetTransfer ---
from model_defs import MobileNetTransfer 

import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import os
import time
from PIL import Image
import warnings

import numpy as np
import random
from typing import List, Tuple

# å¿½ç•¥ PIL/Image åº«å¯èƒ½ç™¼å‡ºçš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# --- 1. é…ç½®èˆ‡åƒæ•¸è¨­å®š (å·²æ›´æ–°) ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_DIR = "cifar10_data" # å°‡æ•¸æ“šä¸‹è¼‰åˆ°æ­¤è³‡æ–™å¤¾
MODEL_SAVE_PATH = "trained_model"
CHECKPOINT_FILE = "latest_checkpoint_cifar10_mobilenet.pth" # æ›´æ”¹æª¢æŸ¥é»æª”æ¡ˆåç¨±
NUM_EPOCHS = 50 # <-- åƒ…è¨“ç·´ 50 å€‹ Epochs
BATCH_SIZE = 32
# é·ç§»å­¸ç¿’æ™‚ï¼Œåªè¨“ç·´åˆ†é¡é ­éƒ¨ï¼Œä½¿ç”¨è¼ƒé«˜çš„å­¸ç¿’ç‡
TRANSFER_LEARNING_LR = 0.001 
# FINE_TUNE_LR = 0.00001 # é€™è£¡åƒ…åšåˆ†é¡é ­è¨“ç·´ï¼Œæš«ä¸ä½¿ç”¨å¾®èª¿ LR

WANT_REPRODUCEBILITY = False    # æ˜¯å¦è¦å¼·åŒ–è¨“ç·´çµæœçš„å¯é‡ç¾æ€§ (Reproducibility)
SEED = 42
USE_PRETRAINED = True           # å¿…é ˆä½¿ç”¨é è¨“ç·´æ¬Šé‡

# ImageNet æ¨™æº–åŒ–åƒæ•¸ (MobileNetV2 æ¨™æº–è¼¸å…¥)
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

# CIFAR-10 é¡åˆ¥åç¨± (10 å€‹é¡åˆ¥)
CIFAR10_CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
NUM_CLASSES = len(CIFAR10_CLASSES)


def set_seed(seed_value=42):
    """ è¨­å®šæ‰€æœ‰éš¨æ©Ÿæ€§çš„ç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾ã€‚ """
    print("è¨­å®šæ‰€æœ‰éš¨æ©Ÿæ€§çš„ç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾ã€‚")
    random.seed(seed_value)         
    np.random.seed(seed_value)      
    torch.manual_seed(seed_value)   
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)      
        torch.backends.cudnn.deterministic = True   
        torch.backends.cudnn.benchmark = False      
    
    def seed_worker(worker_id):
        worker_seed = seed_value + worker_id
        random.seed(worker_seed)
        np.random.seed(worker_seed)
        
    g = torch.Generator()
    g.manual_seed(seed_value)
    
    return seed_worker, g # è¿”å› worker_init_fn å’Œ generator çµ¦ DataLoader ä½¿ç”¨


# --- 2. è¼”åŠ©å‡½å¼ï¼šå‡çµ/è§£å‡æ¨¡å‹åŸºç¤å±¤ ---
def freeze_base_layers(model: MobileNetTransfer, freeze: bool):
    """ å‡çµæˆ–è§£å‡ MobileNetV2 çš„åŸºç¤ç‰¹å¾µæå–å±¤ã€‚ """
    # base_model æ˜¯ MobileNetV2 çš„ç‰¹å¾µæå–éƒ¨åˆ†
    for param in model.base_model.features.parameters():
        param.requires_grad = freeze
    
    # åˆ†é¡å™¨é ­éƒ¨ (classifier) ä¿æŒå¯è¨“ç·´
    for param in model.base_model.classifier.parameters():
        param.requires_grad = True

    if freeze:
        print("ğŸ’¡ æ¨¡å‹åŸºç¤ç‰¹å¾µæå–å±¤å·²å‡çµ (åªè¨“ç·´åˆ†é¡å™¨é ­éƒ¨)ã€‚")
    else:
        print("ğŸ’¡ æ¨¡å‹åŸºç¤ç‰¹å¾µæå–å±¤å·²è§£å‡ (æº–å‚™é€²è¡Œå¾®èª¿/Fine-tuning)ã€‚")


# --- 3. æ•¸æ“šåŠ è¼‰ï¼šä½¿ç”¨å…§å»º CIFAR-10 æ•¸æ“šé›† ---

def get_loaders(data_dir: str, batch_size: int, want_reproducibility: bool, seed: int) -> Tuple[DataLoader, DataLoader, int, List[str]]:
    """è¼‰å…¥ CIFAR-10 æ•¸æ“šé›†ä¸¦å›å‚³ DataLoader å’Œé¡åˆ¥è³‡è¨Šã€‚"""
    
    # è¨“ç·´é›†å°ˆç”¨è½‰æ› (Resize åˆ° 224x224, åŒ…å«æ•¸æ“šæ“´å¢)
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)), 
        transforms.RandomHorizontalFlip(), 
        transforms.ToTensor(), 
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) 
    ])

    # é©—è­‰é›†å°ˆç”¨è½‰æ› 
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)), 
        transforms.ToTensor(), 
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) 
    ])

    worker_init_fn = None
    generator = None
    if want_reproducibility:
        worker_init_fn, generator = set_seed(seed)
    
    # åœ¨ PyTorch ä¸­ï¼Œæ‰€æœ‰çš„ torchvision.datasets ç‰©ä»¶åœ¨å¯¦ä¾‹åŒ–å¾Œï¼Œéƒ½æœƒå…§å»ºå…©å€‹éå¸¸é‡è¦çš„å±¬æ€§ï¼š.classes (æ¸…å–®) å’Œ .class_to_idx (å­—å…¸)ã€‚
    # è‹¥è¦çŸ¥é“ CIFAR-10 çš„é¡åˆ¥åç¨±å’Œç´¢å¼•å°æ‡‰é—œä¿‚ï¼Œä¸éœ€è¦å»è®€æºç¢¼ï¼Œåªéœ€è¦åœ¨ Python äº¤äº’å¼ç’°å¢ƒï¼ˆå¦‚ Jupyter æˆ– Python REPLï¼‰è·‘ä¸‹é¢ç¨‹å¼å³å¯ï¼š
    #   from torchvision import datasets
    #   train_data = datasets.CIFAR10(root="data", train=True, download=True)
    #   print(train_data.classes)       # å°å‡ºæ¨™ç±¤å­—ä¸²æ¸…å–®
    #   print(train_data.class_to_idx)  # å°å‡º {æ¨™ç±¤: ç´¢å¼•} çš„å°ç…§è¡¨
    #
    # å¦å¤–ç„¡è«–æ˜¯torch, tensorflow/keras, éƒ½å¯ä»¥ä½¿ç”¨pythonå…§å»ºçš„ dir()å‡½å¼ä¾†æŸ¥çœ‹ dataset ç‰©ä»¶çš„æ‰€æœ‰å±¬æ€§å’Œæ–¹æ³•ï¼š
    #   print(dir(train_data))

    # è¼‰å…¥è¨“ç·´é›†
    # å¯ä»¥ç”¨ä¸‹é¢æ–¹å¼çœ‹çœ‹ä¸€å€‹sampleæœ‰å“ªäº›æ¬„ä½. 
    #   img, label = dataset[0]
    #   print(f"Image shape: {getattr(img, 'size', 'N/A')}, Label: {label}")    # getattr(object, name[, default])
    train_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_transform)
    # è¼‰å…¥é©—è­‰é›†
    val_dataset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=val_transform)

    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=4, 
        worker_init_fn=worker_init_fn if want_reproducibility else None, 
        generator=generator if want_reproducibility else None
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=4,
        generator=generator if want_reproducibility else None
    )

    return train_loader, val_loader, NUM_CLASSES, list(CIFAR10_CLASSES)


def calculate_accuracy(loader, model):
    # ä¿æŒä¸è®Š
    model.eval() 
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total


def save_checkpoint(epoch, model, optimizer, best_acc, path):
    # ä¿æŒä¸è®Š
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_accuracy': best_acc,
        'timestamp': time.strftime("%Y%m%d-%H%M%S")
    }
    torch.save(checkpoint, path)
    print(f"\n[CHECKPOINT] ç‹€æ…‹å·²å„²å­˜åˆ° {path} (Epoch: {epoch}, Acc: {best_acc:.2f}%)")

def load_checkpoint(path, model, optimizer):
    # ä¿æŒä¸è®Š
    if not os.path.exists(path):
        return 0, 0.0, False 

    checkpoint = torch.load(path, map_location=device)
    
    try:
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_accuracy = checkpoint.get('best_accuracy', 0.0)
        
        print(f"\n[CHECKPOINT] å·²è¼‰å…¥æª¢æŸ¥é»ï¼Œå¾ Epoch {start_epoch} æ¢å¾©è¨“ç·´ (æ­·å²æœ€ä½³ Acc: {best_accuracy:.2f}%)")
        return start_epoch, best_accuracy, True
        
    except Exception as e:
        print(f"[è­¦å‘Š] æª¢æŸ¥é»è¼‰å…¥å¤±æ•—: {e}ã€‚å°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚")
        return 0, 0.0, False

# --- 4. è¨“ç·´æµç¨‹ä¸»å‡½å¼ (å·²ä¿®æ­£å„ªåŒ–å™¨å’Œ LR é‚è¼¯) ---
def train_model(train_loader, val_loader, model, total_epochs, start_epoch, initial_best_acc):
    
    # ç”±æ–¼åŸºç¤å±¤å‡çµï¼Œå„ªåŒ–å™¨åªå„ªåŒ– requires_grad=True (å³åˆ†é¡å™¨é ­éƒ¨) çš„åƒæ•¸
    # ä½¿ç”¨ TRANSFER_LEARNING_LR é€²è¡Œè¨“ç·´
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=TRANSFER_LEARNING_LR
    )
    
    criterion = nn.CrossEntropyLoss()
    
    best_accuracy = initial_best_acc 
    visual_best_acc = best_accuracy
    
    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
    checkpoint_path = os.path.join(MODEL_SAVE_PATH, CHECKPOINT_FILE)
    
    # --- å„²å­˜é‚è¼¯çš„èµ·å§‹ Epoch ---
    # ç”±æ–¼åªæœ‰ 50 å€‹ Epochï¼Œåœ¨æœ€å¾Œ 10 å€‹ Epoch é–‹å§‹å„²å­˜
    saving_start_epoch = total_epochs - 10 

    # å¦‚æœæ˜¯å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´ï¼Œéœ€è¦é‡æ–°è¨­å®šå­¸ç¿’ç‡
    if start_epoch > 0:
        for param_group in optimizer.param_groups:
            # ç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„é·ç§»å­¸ç¿’ LR
            param_group['lr'] = TRANSFER_LEARNING_LR
        print(f"[çºŒè¨“] è¨­ç½®ç•¶å‰å­¸ç¿’ç‡ç‚º {optimizer.param_groups[0]['lr']}")
    
    print(f"\n--- é–‹å§‹é·ç§»å­¸ç¿’ (ç¸½ç›®æ¨™ Epoch: {total_epochs}, å¾ Epoch {start_epoch + 1} é–‹å§‹) ---")
    print(f"è¨“ç·´æ¨¡å¼: åŸºç¤å±¤å·²å‡çµï¼Œåªè¨“ç·´åˆ†é¡å™¨é ­éƒ¨ (LR={TRANSFER_LEARNING_LR})ã€‚")
    print(f"æ³¨æ„: æ¨¡å‹å„²å­˜åŠŸèƒ½å°‡åœ¨ç¬¬ {saving_start_epoch + 1} å€‹ Epoch å•Ÿå‹•ã€‚")
    
    try:
        for epoch in range(start_epoch, total_epochs):
            current_epoch_num = epoch + 1
            
            # ç¢ºä¿åœ¨è¨“ç·´é–‹å§‹æ™‚æ¨¡å‹æ˜¯å‡çµç‹€æ…‹ (åªè¨“ç·´æ–°åŠ çš„åˆ†é¡é ­éƒ¨)
            model.train() 
            
            # --- è¨“ç·´éšæ®µ ---
            running_loss = 0.0
            pbar = tqdm(train_loader, desc=f"Epoch {current_epoch_num}/{total_epochs}", leave=False)
            for images, labels in pbar:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item() * images.size(0)
                pbar.set_postfix({'loss': loss.item()})

            train_loss = running_loss / len(train_loader.dataset)
            
            # --- é©—è­‰éšæ®µ ---
            model.eval() 
            val_loss = 0.0
            with torch.no_grad():
                for images, labels in val_loader:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item() * images.size(0)
            
            val_loss /= len(val_loader.dataset)
            val_accuracy = calculate_accuracy(val_loader, model)
            
            # --- å„²å­˜æª¢æŸ¥é»é‚è¼¯ ---
            print_message = f"Epoch {current_epoch_num}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%"
            
            if val_accuracy > visual_best_acc:
                visual_best_acc = val_accuracy
                print_message += f" (æ–°æ­·å²æœ€é«˜: {visual_best_acc:.2f}%)"
            
            is_saving_epoch = current_epoch_num >= saving_start_epoch
            
            if is_saving_epoch:
                if val_accuracy > best_accuracy:
                    best_accuracy = val_accuracy
                    save_checkpoint(epoch, model, optimizer, best_accuracy, checkpoint_path)
                    print_message += f" -> **æ¨¡å‹ç‹€æ…‹å·²æ›´æ–°å„²å­˜** (ç›®å‰æœ€ä½³ Acc: {best_accuracy:.2f}%)"
                else:
                    print_message += f" (å„²å­˜å€é–“å…§ï¼Œç›®å‰æœ€ä½³: {best_accuracy:.2f}%)"
            
            else:
                print_message += f" (å„²å­˜åŠŸèƒ½é—œé–‰ï¼Œå‰©é¤˜ {saving_start_epoch - current_epoch_num} å€‹ Epoch å•Ÿå‹•)"

            print(print_message)
            
    except KeyboardInterrupt:
        print("\n\n*** [ä½¿ç”¨è€…ä¸­æ–·] åµæ¸¬åˆ° Ctrl+Cï¼Œæå‰çµæŸè¨“ç·´ã€‚ ***")
        try:
             save_checkpoint(epoch, model, optimizer, best_accuracy, checkpoint_path)
        except NameError:
             print("-> ç„¡æ³•å„²å­˜ï¼Œå› ç‚ºç•°å¸¸ç™¼ç”Ÿåœ¨ç¬¬ä¸€å€‹ Epoch é–‹å§‹ä¹‹å‰ã€‚")
        except Exception as save_err:
             print(f"-> å„²å­˜æª¢æŸ¥é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {save_err}")

    except RuntimeError as e:
        if "DataLoader worker" in str(e):
             print("\n\n*** [DataLoaderä¸­æ–·] åµæ¸¬åˆ° DataLoader worker ç•°å¸¸é€€å‡º (å¯èƒ½ç”± Ctrl+C å¼•èµ·)ã€‚ ***")
             try:
                 save_checkpoint(epoch, model, optimizer, best_accuracy, checkpoint_path)
                 print(f"-> æˆåŠŸå„²å­˜æª¢æŸ¥é»ï¼Œä»¥é˜²æ•¸æ“šä¸Ÿå¤±ã€‚")
             except NameError:
                 print("-> ç„¡æ³•å„²å­˜ï¼Œå› ç‚ºç•°å¸¸ç™¼ç”Ÿåœ¨ç¬¬ä¸€å€‹ Epoch é–‹å§‹ä¹‹å‰ã€‚")
             except Exception as save_err:
                 print(f"-> å„²å­˜æª¢æŸ¥é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {save_err}")
        else:
            raise e
            
    finally:
        print("-" * 50)
        print(f"è¨“ç·´æµç¨‹çµæŸã€‚")
        print(f"æ•´é«”è¨“ç·´éç¨‹ä¸­çš„æœ€é«˜æº–ç¢ºåº¦: {visual_best_acc:.2f}%\n")
        if best_accuracy > 0.0:
            print(f"æœ€çµ‚å„²å­˜çš„æœ€ä½³æº–ç¢ºåº¦: {best_accuracy:.2f}%")
            
# --- 5. åŸ·è¡Œå€å¡Š ---
if __name__ == '__main__':
    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
    checkpoint_path = os.path.join(MODEL_SAVE_PATH, CHECKPOINT_FILE)

    try:
        train_loader, val_loader, num_classes_detected, class_names_detected = get_loaders(
            DATA_DIR, BATCH_SIZE, WANT_REPRODUCEBILITY, SEED
        )
        
        print(f"ç¸½è¨“ç·´æ¨£æœ¬æ•¸: {len(train_loader.dataset)}")
        print(f"ç¸½é©—è­‰æ¨£æœ¬æ•¸: {len(val_loader.dataset)}")
        print(f"åµæ¸¬åˆ°é¡åˆ¥æ•¸é‡: {num_classes_detected}")
        print(f"é¡åˆ¥åç¨±: {class_names_detected}")
        
        # æ­¥é©Ÿ 1: åˆå§‹åŒ–æ¨¡å‹ (ä½¿ç”¨é è¨“ç·´ MobileNetV2)
        model = MobileNetTransfer(num_classes=num_classes_detected, use_pretrained=USE_PRETRAINED).to(device)
        # åˆå§‹åŒ–å„ªåŒ–å™¨ï¼ˆç”¨æ–¼ load_checkpoint è¼‰å…¥ç‹€æ…‹ï¼Œä½¿ç”¨é·ç§»å­¸ç¿’ LRï¼‰
        initial_optimizer = optim.Adam(model.parameters(), lr=TRANSFER_LEARNING_LR) 

        # æ­¥é©Ÿ 2: è¼‰å…¥æª¢æŸ¥é»
        start_epoch, best_accuracy, is_resumed = load_checkpoint(checkpoint_path, model, initial_optimizer)
        
        # æ­¥é©Ÿ 3: ç¢ºä¿æ¨¡å‹å‡çµç‹€æ…‹æ­£ç¢º (é·ç§»å­¸ç¿’çš„é—œéµæ­¥é©Ÿ)
        # åœ¨åªè¨“ç·´åˆ†é¡é ­éƒ¨çš„éšæ®µï¼Œç¢ºä¿åŸºç¤å±¤æ˜¯å‡çµçš„
        freeze_base_layers(model, freeze=True)

        # æ­¥é©Ÿ 4: é–‹å§‹è¨“ç·´ 
        train_model(train_loader, val_loader, model, NUM_EPOCHS, start_epoch, best_accuracy)
        
    except ValueError as e:
        print(f"\n[è³‡æ–™éŒ¯èª¤] {e}\nè«‹æª¢æŸ¥ {DATA_DIR} ç›®éŒ„ï¼Œæˆ–ç¢ºèªä¸‹è¼‰æ˜¯å¦æˆåŠŸã€‚")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n[ä¸€èˆ¬éŒ¯èª¤] è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")