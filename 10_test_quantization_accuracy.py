import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import os
import time
from typing import List, Tuple

# --- å°å…¥ torchao çš„é‡åŒ– API ---
from torchao.quantization import quantize_ # æœ€ä¸Šå±¤çš„é‡åŒ–å‡½å¼
from torchao.quantization import Int8DynamicActivationInt8WeightConfig # Int8 å‹•æ…‹é‡åŒ–é…ç½®
# -----------------------------

# --- å¾ model_defs æ¨¡çµ„å°å…¥æ¨¡å‹çµæ§‹ ---
from model_defs import MobileNetTransfer 

# --- 1. é…ç½®èˆ‡åƒæ•¸è¨­å®š (èˆ‡è¨“ç·´/é‡åŒ–æ™‚ä¿æŒä¸€è‡´) ---
# æ¸¬è©¦é€šå¸¸åœ¨ CPU ä¸Šé€²è¡Œï¼Œä»¥æ¨¡æ“¬éƒ¨ç½²ç’°å¢ƒ
DEVICE = torch.device("cpu") 
DATA_DIR = "cifar10_data" 
MODEL_SAVE_PATH = "trained_model"
BATCH_SIZE = 64 # æ¸¬è©¦æ™‚å¯ä»¥é©ç•¶æé«˜æ‰¹æ¬¡å¤§å°

# FP32 æ¨¡å‹é…ç½®
FP32_CHECKPOINT_FILE = "latest_checkpoint_cifar10_mobilenet.pth"
FP32_CHECKPOINT_PATH = os.path.join(MODEL_SAVE_PATH, FP32_CHECKPOINT_FILE)

# INT8 æ¨¡å‹é…ç½®
INT8_MODEL_FILE = "quantized_mobilenet_cifar10_int8.pth"
INT8_MODEL_PATH = os.path.join(MODEL_SAVE_PATH, INT8_MODEL_FILE)

NUM_CLASSES = 10 
# ImageNet æ¨™æº–åŒ–åƒæ•¸ (MobileNetV2 æ¨™æº–è¼¸å…¥)
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]


# --- 2. æ•¸æ“šåŠ è¼‰ ---
def get_validation_loader(data_dir: str, batch_size: int) -> DataLoader:
    """è¼‰å…¥ CIFAR-10 é©—è­‰é›† DataLoaderã€‚"""
    
    # é©—è­‰é›†å°ˆç”¨è½‰æ› (èˆ‡è¨“ç·´æ™‚é©—è­‰é›†çš„è½‰æ›å¿…é ˆä¸€è‡´)
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)), 
        transforms.ToTensor(), 
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) 
    ])

    # è¼‰å…¥é©—è­‰é›†
    val_dataset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=val_transform)

    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=4 
    )
    
    return val_loader

# --- 3. æº–ç¢ºåº¦è¨ˆç®—å‡½å¼ ---
def calculate_accuracy(loader: DataLoader, model: nn.Module) -> float:
    """ è¨ˆç®—æ¨¡å‹åœ¨ DataLoader ä¸Šçš„æº–ç¢ºåº¦ã€‚ """
    
    # ç¢ºä¿æ¨¡å‹åœ¨è©•ä¼°æ¨¡å¼
    model.eval() 
    correct = 0
    total = 0
    
    # ç¦ç”¨æ¢¯åº¦è¨ˆç®—
    with torch.no_grad():
        start_time = time.time()
        for images, labels in loader:
            # æ¨è«–æ™‚å¿…é ˆå°‡è³‡æ–™ç§»åˆ°æ¨¡å‹æ‰€åœ¨çš„ DEVICE (é€™è£¡ç‚º CPU)
            images, labels = images.to(DEVICE), labels.to(DEVICE) 
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        end_time = time.time()
        
    accuracy = 100 * correct / total
    inference_time = end_time - start_time
    
    return accuracy, inference_time


# --- 4. æ¨¡å‹è¼‰å…¥å‡½å¼ï¼šFP32 åŸºæº–æ¨¡å‹ ---
def load_fp32_model(num_classes: int) -> MobileNetTransfer:
    """è¼‰å…¥ FP32 æ¨¡å‹çµæ§‹å’Œæ¬Šé‡ã€‚"""
    
    model = MobileNetTransfer(num_classes=num_classes, use_pretrained=False) 
    model.to(DEVICE)

    if not os.path.exists(FP32_CHECKPOINT_PATH):
        raise FileNotFoundError(f"éŒ¯èª¤: æ‰¾ä¸åˆ° FP32 æª¢æŸ¥é»æª”æ¡ˆ {FP32_CHECKPOINT_PATH}ã€‚è«‹å…ˆé‹è¡Œè¨“ç·´è…³æœ¬ã€‚")
        
    checkpoint = torch.load(FP32_CHECKPOINT_PATH, map_location=DEVICE)
    
    try:
        model.load_state_dict(checkpoint['model_state_dict'])
    except Exception as e:
        raise ValueError(f"FP32 æ¬Šé‡è¼‰å…¥å¤±æ•—: {e}")

    model.eval() 
    return model


# --- 5. æ¨¡å‹è¼‰å…¥å‡½å¼ï¼šINT8 é‡åŒ–æ¨¡å‹ ---
def load_int8_model(num_classes: int) -> nn.Module:
    """è¼‰å…¥ INT8 æ¨¡å‹çµæ§‹å’Œæ¬Šé‡ã€‚
    
    é‡åŒ–æ¨¡å‹çš„è¼‰å…¥æµç¨‹æ˜¯ï¼š
    1. åˆå§‹åŒ–åŸå§‹ FP32 æ¨¡å‹çµæ§‹ã€‚
    2. å°‡è©²çµæ§‹è½‰æ›ç‚º INT8 é‡åŒ–çµæ§‹ (ä½¿ç”¨ quantize_dynamic)ã€‚
    3. è¼‰å…¥ä¿å­˜çš„ INT8 æ¬Šé‡ (state_dict) åˆ°é‡åŒ–çµæ§‹ä¸­ã€‚
    """
    
    if not os.path.exists(INT8_MODEL_PATH):
        raise FileNotFoundError(f"éŒ¯èª¤: æ‰¾ä¸åˆ° INT8 æ¨¡å‹æª”æ¡ˆ {INT8_MODEL_PATH}ã€‚è«‹å…ˆé‹è¡Œé‡åŒ–è…³æœ¬ 12_quantize_model.pyã€‚")
        
    # æ­¥é©Ÿ 1: åˆå§‹åŒ–åŸå§‹æ¨¡å‹çµæ§‹ (FP32)
    fp32_model = MobileNetTransfer(num_classes=num_classes, use_pretrained=False) 
    
    # æ­¥é©Ÿ 2: å®šç¾©é‡åŒ–é…ç½® (èˆ‡å„²å­˜æ™‚å¿…é ˆä¸€è‡´)
    quant_config = Int8DynamicActivationInt8WeightConfig()
    
    # æ­¥é©Ÿ 3: å°‡çµæ§‹è½‰æ›ç‚ºé‡åŒ–æ¨¡å‹
    # ä½¿ç”¨ quantize_ é€²è¡Œ in-place è½‰æ›
    quantize_(fp32_model, quant_config)
    quantized_model = fp32_model 
    
    # æ­¥é©Ÿ 4: è¼‰å…¥ INT8 æ¬Šé‡
    int8_state_dict = torch.load(INT8_MODEL_PATH, map_location=DEVICE)

    try:
        quantized_model.load_state_dict(int8_state_dict)
    except Exception as e:
        raise ValueError(f"INT8 æ¬Šé‡è¼‰å…¥å¤±æ•—ã€‚é‡åŒ–/è¼‰å…¥çµæ§‹å¯èƒ½ä¸åŒ¹é…: {e}")
        
    quantized_model.eval()
    return quantized_model


# --- 6. ä¸»åŸ·è¡Œå€å¡Š ---
def main():
    print("--- å•Ÿå‹•æ¨¡å‹æº–ç¢ºåº¦èˆ‡æ¨è«–é€Ÿåº¦æ¸¬è©¦ (FP32 vs. INT8) ---")
    
    try:
        # è¼‰å…¥é©—è­‰é›†
        val_loader = get_validation_loader(DATA_DIR, BATCH_SIZE)
        print(f"âœ… è¼‰å…¥ CIFAR-10 é©—è­‰é›† (ç¸½æ¨£æœ¬æ•¸: {len(val_loader.dataset)})")

        # è¼‰å…¥ FP32 æ¨¡å‹
        print("\nâ³ è¼‰å…¥ FP32 åŸºæº–æ¨¡å‹...")
        fp32_model = load_fp32_model(NUM_CLASSES)
        
        # è¼‰å…¥ INT8 æ¨¡å‹
        print("â³ è¼‰å…¥ INT8 é‡åŒ–æ¨¡å‹...")
        int8_model = load_int8_model(NUM_CLASSES)
        
        # --- æ¸¬è©¦ FP32 æ¨¡å‹ ---
        print("\n--- æ¸¬è©¦ FP32 æ¨¡å‹ ---")
        fp32_acc, fp32_time = calculate_accuracy(val_loader, fp32_model)
        
        # --- æ¸¬è©¦ INT8 æ¨¡å‹ ---
        print("--- æ¸¬è©¦ INT8 æ¨¡å‹ ---")
        int8_acc, int8_time = calculate_accuracy(val_loader, int8_model)
        
        # --- è¼¸å‡ºçµæœ ---
        print("\n" + "=" * 40)
        print("     ğŸ”¥ æ¨¡å‹é‡åŒ–æ•ˆæœåˆ†æ (CIFAR-10 é©—è­‰é›†) ğŸ”¥")
        print("=" * 40)
        
        # æº–ç¢ºåº¦å°æ¯”
        print(f"** æº–ç¢ºåº¦ (Accuracy) **")
        print(f"FP32 æ¨¡å‹æº–ç¢ºåº¦: {fp32_acc:.2f}%")
        print(f"INT8 æ¨¡å‹æº–ç¢ºåº¦: {int8_acc:.2f}%")
        
        acc_drop = fp32_acc - int8_acc
        print(f"æº–ç¢ºåº¦æå¤± (Loss): {acc_drop:.2f}%")
        
        # æ¨è«–é€Ÿåº¦å°æ¯”
        print(f"\n** æ¨è«–æ™‚é–“ (Inference Time) ** (ç¸½è€—æ™‚)")
        print(f"FP32 æ¨¡å‹æ¨è«–ç¸½è€—æ™‚: {fp32_time:.4f} ç§’")
        print(f"INT8 æ¨¡å‹æ¨è«–ç¸½è€—æ™‚: {int8_time:.4f} ç§’")
        
        speed_up = fp32_time / int8_time if int8_time > 0 else float('inf')
        print(f"INT8 ç›¸è¼ƒæ–¼ FP32 çš„åŠ é€Ÿæ¯”: {speed_up:.2f} å€")
        print("=" * 40)
        
    except FileNotFoundError as e:
        print(f"\n[è‡´å‘½éŒ¯èª¤] {e}")
        print("è«‹ç¢ºä¿æ‚¨å·²é‹è¡Œè¨“ç·´è…³æœ¬ (8_transfer_train_cifar10.py) å’Œé‡åŒ–è…³æœ¬ (12_quantize_model.py)ã€‚")
    except ValueError as e:
        print(f"\n[è‡´å‘½éŒ¯èª¤] {e}")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n[ä¸€èˆ¬éŒ¯èª¤] ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == '__main__':
    main()